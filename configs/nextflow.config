/*
========================================================================================
    Generic Nextflow config file
========================================================================================
    Default config for all compute environments
----------------------------------------------------------------------------------------
*/

// Manifest providing default pipeline description
// Every pipeline should override this
manifest {
    name            = 'pipeline'
    author          = 'PAM Informatics'
    homePage        = 'NA'
    description     = 'NA'
    mainScript      = 'main.nf'
    version         = 'NA'
}

// Global default params
params {
    // Input options
    input = null

    // Boilerplate options
    outdir = './results'
    tracedir = "${params.outdir}/pipeline_info"
    help = false

    // Max resource options
    // Defaults only, expecting to be overwritten in profiles
    max_memory = '4.GB'
    max_cpus = 2
    max_time = '10000.h'
    max_retries = 2

    // LSF
    queue_size = null
    submit_rate_limit = null
}

// Specify process resource requirements and escalation strategy

/**
* Function to ensure that resource requirements don't go beyond
* a maximum limit
*/
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

def escalate_linear(initial, task, multiplier=1) {
    assert multiplier > 0
    return (initial * multiplier * task.attempt)
}

def escalate_exp(initial, task, multiplier=1) {
    assert multiplier > 0
    return (initial * (multiplier ** (task.attempt - 1)))
}

def retry_strategy(task, max_retries) {
    def MISC_EXIT_CODES = [
        "SIGKILL": 137,
        "SIGTERM": 143,
        "SIGABRT": 134,
        "SIGSEGV": 139
    ].values()

    def SCALING_EXIT_CODES = [
        "SIGINT": 130,  // LSF Out of memory Error or bkill
        "SIGUSR2": 140, // LSF Runlimit exceeded Error
    ].values()

    if (task.attempt > max_retries) {
        return 'ignore'
    }

    switch(task.exitStatus) {
        case {it in MISC_EXIT_CODES}:
            //break due to non-scalable error code (could be 'ignore')
            return 'finish'

        case {it in SCALING_EXIT_CODES}:
            //retry with more memory and longer time limit
            return 'retry'

        default:
            //finish due to other error code not in above list
            return 'finish'
    }
}

process {
    // Defaults
    cpus   = { check_max( escalate_linear( 1, task ), 'cpus' ) }
    memory = { check_max( escalate_linear( 1.GB, task ), 'memory' ) }
    time   = { check_max( escalate_linear( 1.h, task ), 'time' ) }

    maxErrors = -1
    maxRetries = params.max_retries
    errorStrategy = { retry_strategy(task, params.max_retries) }

    // Process-specific resource requirements
    withLabel:process_low {
        cpus   = { check_max( 2, 'cpus') }
        memory = { check_max( escalate_exp( 1.GB, task, 2 ), 'memory' ) }
        time   = { check_max( escalate_exp( 1.h, task, 2 ), 'time' ) }
    }

    withLabel:process_medium {
        cpus   = { check_max( 4, 'cpus') }
        memory = { check_max( escalate_exp( 8.GB, task, 2 ), 'memory' ) }
        time   = { check_max( escalate_exp( 12.h, task, 2 ), 'time' ) }
    }

    withLabel:process_high {
        cpus   = { check_max( 8, 'cpus') }
        memory = { check_max( escalate_exp( 16.GB, task, 2 ), 'memory' ) }
        time   = { check_max( escalate_exp( 12.h, task, 2 ), 'time' ) }
    }

    withLabel:process_long {
        time   = { check_max( escalate_linear( 48.h, task ), 'time' ) }
    }

    withLabel:process_high_memory {
        memory = { check_max( escalate_exp( 32.GB, task, 2 ), 'memory' ) }
    }

    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }
}


profiles {
    debug { process.beforeScript = 'echo $HOSTNAME' }

    docker {
        docker.enabled         = true
        docker.userEmulation   = true
        singularity.enabled    = false
    }

    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        docker.enabled         = false
    }

    // Default profile
    standard {
        docker {
            enabled = false
        }

        singularity {
            enabled = true
            autoMounts = true
            runOptions = '--bind /data,/lustre,/nfs,/software'
            // To avoid downloading/converting to singularity image every time
            libraryDir = '/software/pathogen/custom_installs/singularity_cache'
        }

        process {
            executor = 'lsf'
            queue = { task.memory > 745.GB ? 'teramem' :
                      task.memory > 196.GB ? 'hugemem' :
                      task.time <= 12.h ? 'normal' :
                      task.time <= 48.h ? 'long' :
                      'basement' }
        }

        executor {
            name = 'lsf'
            perJobMemLimit = true
            submitRateLimit = params.submit_rate_limit
            queueSize = params.queue_size
            // pipeline_name is a variable to be overriden pipeline-specific config
            jobName = { "$pipeline_name - $task.name - $task.hash" }
        }

        params {
            // Max resources
            max_memory = 2.9.TB
            max_cpus = 256
            max_time = 720.h
        }
    }
}

// Export these variables to prevent local Python/R libraries from conflicting with those in the container
env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

// Setup reports with uniform filename` 
def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')

timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}

report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}

trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}

dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.svg"
}
